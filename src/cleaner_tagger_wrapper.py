from llama_cpp import Llama

MODEL_PATH = r"C:\Users\ILuna\.lmstudio\models\TheBloke\Mistral-7B-Instruct-v0.1-GGUF\mistral-7b-instruct-v0.1.Q4_K_S.gguf"
# MODEL_PATH = r"C:\Users\ILuna\.lmstudio\models\lmstudio-community\mathstral-7B-v0.1-GGUF\mathstral-7B-v0.1-Q4_K_M.gguf"

llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=8,  # You can bump this up with your Ryzen 7
    n_batch=64
)

def tag_line(text: str) -> str:
    prompt = f"""Classify the following player message as either based on the context of a Dungeons & Dragons game.:
- in-character
- meta

Message: "{text}"
Tag:"""
    response = llm(prompt, max_tokens=10, stop=["\n"])
    output = response["choices"][0]["text"].strip().lower()
    print(f'Tagging with AI: {output}')
    return "meta" if "meta" in output else "in-character"



def clean_text(text: str, max_tokens: int = None) -> str:
    if max_tokens is None:
        word_count = len(text.strip().split())
        max_tokens = min(200, max(40, int(word_count * 2)))  # More breathing room

    print(f"ðŸ§½ Cleaning with max_tokens={max_tokens} for input: {text}")

    prompt = f"""
This message was generated by a speech-to-text system.
Correct only obvious transcription errors (e.g., misheard words, missing punctuation, repeated phrases).
Preserve the original wording, sentence structure, and tone â€” do not rewrite or paraphrase.
Avoid over-correction and do not add or remove meaning. Just make the original message more readable and accurate.
Original: {text}
Cleaned:
"""

    response = llm(prompt, max_tokens=max_tokens)  # removed stop=["\n"]
    cleaned = response["choices"][0]["text"].strip()

    print(f"Text cleaned by AI: {cleaned}")
    return cleaned

